{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a44318-8837-48a4-a20a-78ceb17ebc1d",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2121cdb-53e1-43ca-b48c-1b7022d9ec87",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It builds an ensemble of weak learners, usually decision trees, by sequentially fitting each tree to the residual errors of the previous trees, thus gradually improving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b93f3-be36-4aec-836b-6bc6d4c0e53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9e6b3be-d6d8-42cb-a9fd-cfaef8c42ab8",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86383948-33a2-4b3d-a0cd-92a029eeb70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.86\n",
      "R-squared: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 2.0, 2.5, 3.5, 4.0])\n",
    "\n",
    "class SimpleGBR:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.initial_prediction = np.mean(y)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        residuals = y - self.initial_prediction\n",
    "        for _ in range(self.n_estimators):\n",
    "            model = self._build_tree(X, residuals)\n",
    "            self.models.append(model)\n",
    "            predictions = model(X)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.initial_prediction)\n",
    "        for model in self.models:\n",
    "            y_pred += self.learning_rate * model(X)\n",
    "        return y_pred\n",
    "\n",
    "    def _build_tree(self, X, residuals):\n",
    "        mean_residual = np.mean(residuals)\n",
    "        return lambda x: np.full(x.shape[0], mean_residual)\n",
    "\n",
    "# Train the model\n",
    "gbr = SimpleGBR(n_estimators=10, learning_rate=0.1)\n",
    "gbr.fit(X, y)\n",
    "y_pred = gbr.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59c8c7-5f87-4408-928b-48fbb21226ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b429628b-318d-4c8c-8a34-5f2edf44a9ee",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06a61f42-e366-49a7-a8b3-6c8125f3dede",
   "metadata": {},
   "source": [
    "# To optimize hyperparameters, you can use grid search or random search. Hereâ€™s an example using grid search:\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=3, scoring='r2')\n",
    "grid_search.fit(X, y)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f9531-b7a1-4967-8b93-b69d97a4d75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4c285f6-9b1f-492d-8609-4acef327c9a6",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8062055-89e8-400c-8c2b-6bdf566fab5f",
   "metadata": {},
   "source": [
    "A weak learner in Gradient Boosting is a simple model that performs slightly better than random guessing. Typically, decision trees with shallow depth are used as weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba64e44-ca31-4655-9cef-984028866ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39b65e2b-0d93-4862-8951-82ce55113ddd",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3222925c-ae11-4ce7-b52e-4d9415914e66",
   "metadata": {},
   "source": [
    "The intuition behind Gradient Boosting is to iteratively add models that correct the errors of the combined ensemble. Each new model is trained to predict the residual errors (gradients) of the current ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b391f-28a7-4cef-a774-6be5b595d968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b40e8f4-05fa-4e4c-9e6b-9f6439597376",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ffa4d-41ee-4e02-a7fe-b96f70ddc7e7",
   "metadata": {},
   "source": [
    "Gradient Boosting builds an ensemble by sequentially adding weak learners, each trained to minimize the residual errors of the combined predictions of the previous models. The predictions of these learners are weighted by a learning rate and summed to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf6f8b-7fa4-46a2-a8e0-3d70758c6a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3b1d629-c5df-408d-a80c-6e37a0ce157e",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df707423-a559-41ec-bb9c-d58f757cd7b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
